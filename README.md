# bot-conversa-ai ğŸ¤–ğŸ’¬

O **bot-conversa-ai** Ã© uma plataforma local de atendimento com InteligÃªncia Artificial,
voltada para conversas institucionais via mÃºltiplos canais (como WhatsApp e Telegram),
utilizando **LLMs open-source rodando localmente**, com foco em controle, seguranÃ§a e rastreabilidade.

O projeto foi concebido para ambientes que exigem **autonomia tecnolÃ³gica**, **auditoria** e
**execuÃ§Ã£o local**, como Ã³rgÃ£os pÃºblicos, instituiÃ§Ãµes e ambientes corporativos.

---

## ğŸ¯ Objetivo do Projeto

- Realizar **atendimento inicial automatizado**
- Fornecer **informaÃ§Ãµes institucionais bÃ¡sicas**
- Orientar usuÃ¡rios de forma clara e padronizada
- Encaminhar demandas para atendimento humano quando necessÃ¡rio
- Registrar conversas para fins administrativos e auditoria
- Operar **sem dependÃªncia de APIs externas**

---

## ğŸ§  CaracterÃ­sticas Principais

- ExecuÃ§Ã£o **100% local / self-hosted**
- Uso de **LLMs open-source** via Ollama
- OrquestraÃ§Ã£o de fluxos com **n8n**
- PersistÃªncia de dados em **PostgreSQL**
- Interface administrativa via **PHP (Adianti Framework)**
- Arquitetura modular e escalÃ¡vel
- Suporte a mÃºltiplos canais de entrada
- Postura institucional e controle de respostas

---

## ğŸ§© Arquitetura

UsuÃ¡rio
â†“
Canal de Entrada (Webhook / WhatsApp / Telegram)
â†“
n8n (orquestraÃ§Ã£o e regras)
â†“
Ollama (LLM local)
â†“
Resposta ao usuÃ¡rio
â†“
PostgreSQL (registro de conversas)
â†“
Painel Administrativo



---

## ğŸš€ Stack TecnolÃ³gica

- Docker / Docker Compose
- Ollama (LLM local)
- Modelo LLM: **phi3**
- n8n (workflow e automaÃ§Ã£o)
- PostgreSQL (persistÃªncia e auditoria)
- Open WebUI (interface opcional para testes do Ollama)
- PHP + Adianti Framework (gestÃ£o administrativa)

---

## ğŸ“‹ PrÃ©-requisitos

Antes de iniciar, certifique-se de ter instalado:

- Docker
- Docker Compose (plugin ou standalone)
- Git

---

## ğŸ“¥ Clonando o RepositÃ³rio

```bash
git clone https://github.com/leandroluz/bot-conversa-ai.git
cd bot-conversa-ai 
```
---

## â–¶ï¸ Subindo os Containers

Execute o comando abaixo para iniciar todos os serviÃ§os:
```bash
docker compose up -d
```

Isso irÃ¡ iniciar:

Ollama (LLM local)

n8n (orquestraÃ§Ã£o)

PostgreSQL (banco de dados)

Open WebUI (interface opcional)

Painel Administrativo (Adianti)

--- 

## ğŸ§° Painel Administrativo (Adianti)

O painel Adianti foi incorporado ao projeto em `admin-panel/` e usa o **mesmo PostgreSQL**
do serviÃ§o principal (banco `atendente`). Portanto, **nÃ£o use** o `docker-compose.yml`
interno do `admin-panel`.

### ğŸŒ Acesso
ApÃ³s subir os containers, acesse:

```
http://localhost:8081
```

### ğŸ§± InicializaÃ§Ã£o do banco do Adianti
O Adianti precisa de algumas tabelas base. Para inicializar, rode:

```bash
for f in admin-panel/app/database/*.sql; do
  docker exec -i postgres psql -U atendente -d atendente < "$f"
done
```


## ğŸ§  InstalaÃ§Ã£o do Modelo LLM (OBRIGATÃ“RIO)

ApÃ³s subir os containers, Ã© necessÃ¡rio baixar manualmente o modelo LLM
utilizado pelo Ollama.

Execute uma Ãºnica vez:

```bash
docker exec -it ollama ollama pull phi3

```

Esse comando irÃ¡:

baixar o modelo phi3

armazenÃ¡-lo de forma persistente no volume do Ollama

disponibilizÃ¡-lo para uso pelo n8n, Open WebUI e API

---

## âš ï¸ Importante
O projeto nÃ£o baixa modelos automaticamente durante o build
para evitar imagens Docker muito grandes e demoradas.

---

## âœ”ï¸ VerificaÃ§Ã£o do Modelo (Opcional)

Para confirmar que o modelo foi instalado corretamente:

```bash

docker exec -it ollama ollama list
```

SaÃ­da esperada:

NAME         SIZE
phi3:latest  2.2 GB

---
